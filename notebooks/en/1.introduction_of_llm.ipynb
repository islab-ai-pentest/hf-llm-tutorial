{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff835631-b47e-47f4-bd85-e807e324cd7d",
   "metadata": {},
   "source": [
    "# Introduction: Understanding Large Language Models with HuggingFace\r\n",
    "\r\n",
    "## Overview\r\n",
    "\r\n",
    "Large Language Models (LLMs) are advanced deep learning models that revolutionize Natural Language Processing (NLP) by understanding and generating human-like text. In this tutorial, we'll explore the fundamentals of **Llama-2-Ko**, a base model designed for text generation using the Huging Face framework.\r\n",
    "\r\n",
    "### Key Points:\r\n",
    "\r\n",
    "- **Definition:** LLMs leverage neural networks to understand and generate text, capturing language patterns, context, and semantics through vast training datasets.\r\n",
    "\r\n",
    "- **Applications:** LLMs find applications in various NLP tasks, including text generation, summarization, translation, question answering, and sentiment analysis.\r\n",
    "\r\n",
    "## Text Generation Models\r\n",
    "\r\n",
    "Text generation models, a common architecture of LLMs, specialize in producing coherent and contextually relevant text. They leverage deep learning techniques to understand language intricacies and generate human-like responses.\r\n",
    "\r\n",
    "### GPT Models:\r\n",
    "\r\n",
    "- **Architecture:** GPT models use a transformer architecture, capturing long-range dependencies and contextual information effectively through attention mechanisms, self-attention layers, and positional encoding.\r\n",
    "\r\n",
    "- **Working Principle:** Trained in an **unsupervised manner** on massive text data, GPT models learn to **predict the next word** in a sequence, leading to a rich understanding of language patterns.\r\n",
    "\r\n",
    "In this tutorial, we'll focus on working with the Llama-2-Ko **base model**, which is not specifically tuned for instructions or chat interactions. Subsequent tutorials will explore the intricacies of **instruction-tuned and chat-tuned LLMs**. Let's dive into the practical aspects of building a chatbot using this base model.\r\n",
    "ext generation models.t generation models.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed1482d-9ad7-49cb-ba8b-13a5f1f63109",
   "metadata": {},
   "source": [
    "# Practical Guide: Text Generation with HuggingFace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cea130-2358-41ee-9c41-d79deda0f014",
   "metadata": {},
   "source": [
    "In this section, we'll walk through the practical steps of using Hugging Face for text generation. We'll focus on leveraging the GPT models for creating coherent and contextually relevant text. Follow along with the code snippets to understand the implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6b0543-30c9-4321-a76a-11b382b7b22f",
   "metadata": {},
   "source": [
    "## Step 1: Install HuggingFace Transformers Library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9945f0-e5bd-4c59-bc22-24d26f71c9bf",
   "metadata": {},
   "source": [
    "To get started, we need to install the Hugging Face Transformers library, a comprehensive toolkit for natural language processing tasks. Execute the following command in a code cell to install the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32491600-2341-44cf-850f-1ec419ba8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e207a-f6d9-43bb-bccc-403fe4a8aac6",
   "metadata": {},
   "source": [
    "## Step 2: Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ded4c20-7929-484c-a569-577101c47bff",
   "metadata": {},
   "source": [
    "To use Hugging Face's powerful Transformers library, we need to import essential components. In particular, we will make use of two key classes: `AutoModelForCausalLM` and `AutoTokenizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7506ffee-3428-4381-8fe1-0e8b9d9ab5f3",
   "metadata": {},
   "source": [
    "### AutoModelForCausalLM\n",
    "\n",
    "`AutoModelForCausalLM` is a class that provides a simple interface for loading pre-trained causal language models. A causal language model is designed for auto-regressive tasks, where the order of the sequence matters, such as text generation. This class automatically identifies the appropriate model architecture based on the provided model name, making it easy to experiment with different models without changing your code.\n",
    "\n",
    "In the context of our tutorial, we use it to load a pre-trained GPT (Generative Pre-trained Transformer) model for text generation. The `from_pretrained` method loads the model weights, and the `AutoModelForCausalLM` class ensures compatibility with the specific architecture.\n",
    "\n",
    "### AutoTokenizer\n",
    "\n",
    "`AutoTokenizer` is another crucial component from Hugging Face's Transformers library. Tokenizers are essential for processing raw text into a format that can be fed into the model. The `AutoTokenizer` class automatically selects the appropriate tokenizer based on the provided model name.\n",
    "\n",
    "In our tutorial, we use it to load the tokenizer corresponding to the GPT model we selected. This tokenizer is responsible for breaking down input text into tokens, which are then processed by the model. The `from_pretrained` method loads the pre-trained tokenizer associated with the chosen GPT model.\n",
    "\n",
    "By using `AutoModelForCausalLM` and `AutoTokenizer`, we can easily work with various pre-trained language models without having to worry about the specifics of each model's architecture or tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a6bcc88-1427-4cff-8a05-f363580be6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniforge3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a53cb5-8d1c-42c7-87db-727cd5e5c6b8",
   "metadata": {},
   "source": [
    "## Step 3: Load a Pre-trained GPT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b4f4be-576c-4dfb-854b-83d7fa8d8968",
   "metadata": {},
   "source": [
    "Now, let's proceed to load a pre-trained GPT model for text generation. In this tutorial, we'll use the \"beomi/llama-2-ko-7b\" model, an advanced iteration of Llama-2 tailored for Korean text generation. Here's a brief overview of the model:\n",
    "\n",
    "### Llama-2-Ko Overview\n",
    "\n",
    "- **Developer:** Junbum Lee (Beomi)\n",
    "- **Variations:** Llama-2-Ko comes in different parameter sizes, including 7B (used in this tutorial), 13B, and 70B, as well as pretrained and fine-tuned variations.\n",
    "- **Input and Output:** The model takes input text and generates text as output, making it suitable for various text generation tasks.\n",
    "\n",
    "### Vocabulary Expansion\n",
    "\n",
    "- **Original Llama-2:** Vocabulary size of 32,000 using Sentencepiece BPE.\n",
    "- **Expanded Llama-2-Ko:** Vocabulary size increased to 46,336, incorporating Korean vocabulary and merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49605281-0414-41aa-b38f-cbce2f7cc33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [00:02<00:00,  5.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model Name from HuggingFace Hub\n",
    "model_name = \"beomi/llama-2-ko-7b\"\n",
    "\n",
    "# Load Model and Tokenizers\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34975a2e-dd2c-4e52-a21f-2e9ce71bd395",
   "metadata": {},
   "source": [
    "## Step 4: Tokenize Inputs\n",
    "\n",
    "Tokenization is a crucial step in natural language processing that involves breaking down a sequence of text into smaller units, known as tokens. These tokens serve as the fundamental building blocks for language models.\n",
    "\n",
    "Let's walk through the tokenization example with the sentence \"안녕하세요, 오늘은 날씨가 좋네요.\" using Llama-2-Ko:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d57fd0-10b0-4181-bcdc-88e4505e0a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize sentence: ['▁안녕', '하세요', ',', '▁오늘은', '▁날', '씨가', '▁좋네요', '.']\n",
      "Encoded tokens tensor([[    1, 43116, 33055, 29892, 41636, 32502, 33876, 32864, 29889]])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentence\n",
    "tokens = tokenizer.tokenize(\"안녕하세요, 오늘은 날씨가 좋네요.\") # Hello, the weather is nice today.\n",
    "print(\"Tokenize sentence:\", tokens)\n",
    "\n",
    "# Tokenize the sentence into Pytorch Tensor as input id.\n",
    "tokens_pt = tokenizer(\"안녕하세요, 오늘은 날씨가 좋네요.\", return_tensors=\"pt\")\n",
    "print(\"Encoded tokens\", tokens_pt['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef6dd77-d5c9-4016-bc27-a9809ea85f52",
   "metadata": {},
   "source": [
    "In this code snippet:\n",
    "\n",
    "- `tokenizer.tokenize(\"안녕하세요, 오늘은 날씨가 좋네요.\")`: Tokenizes the input sentence into a list of tokens. The resulting list, tokens, represents how the input is broken down into individual units.\n",
    "\n",
    "- `tokenizer(\"안녕하세요, 오늘은 날씨가 좋네요.\", return_tensors=\"pt\")`: Tokenizes the input sentence and returns a PyTorch Tensor containing the input IDs. The Tensor, accessed using `tokens_pt['input_ids']`, provides a numerical representation of the tokens. This is the format suitable for input into the Llama-2-Ko model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f11c93-1780-40ee-b670-7cd25116310b",
   "metadata": {},
   "source": [
    "## Step 5: Generate Text (Next Token Prediction)\n",
    "\n",
    "Now that we have tokenized our input, let's generate text using the Llama-2-Ko model. In this step, we'll perform next token prediction, where the model predicts the next token in the sequence given the input tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96301be2-8f56-43a3-ac1e-3f7a089dec46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output (IDs): tensor([[    1, 43116, 33055, 29892, 41636, 32502, 33876, 32864, 29889, 29871,\n",
      "         30166, 35715, 31354, 34333, 35874, 32500, 31286, 34595, 31435, 35362,\n",
      "         32349, 32869, 29889, 29871, 30166, 34120,   525, 33139, 32019, 30906,\n",
      "         32140, 32921, 32305, 29915, 32295, 32500, 32214, 29889, 29871, 30166,\n",
      "         30393, 32500, 31354, 34333, 29871, 29906, 29900, 29896, 29953, 31571,\n",
      "         29871, 29896, 36648, 34156, 34537, 32500, 32214, 29889, 29871, 30166,\n",
      "         31607, 33070, 31054, 35323, 29871, 29906, 29900, 30890, 34685, 43505,\n",
      "         29892, 29871, 30166, 44897, 31299, 33787, 31286, 33889, 32504, 43143,\n",
      "         33406, 33115, 32757, 32043, 29889, 29871, 30166, 31607, 33070, 31054,\n",
      "         35323,   525, 35356, 32153, 32624, 39486, 32474, 17901, 32295, 34963]])\n",
      "Generated text: 안녕하세요, 오늘은 날씨가 좋네요. ​오늘은 제가 좋아하는 책을 소개해드리려고 합니다. ​바로 '나는 나로 살기로 했다'라는 책입니다. ​이 책은 제가 2016년 1월에 읽었던 책입니다. ​그 당시에 저는 20대 후반이었고, ​직장생활을 하면서 많은 스트레스를 받고 있었습니다. ​그 당시에 저는 '내가 왜 이렇게 살아야 하지?'라는 생각을\n"
     ]
    }
   ],
   "source": [
    "# Generate text using next token prediction\n",
    "output = model.generate(input_ids=tokens_pt['input_ids'], max_length=100,)\n",
    "print(\"Generated output (IDs):\", output)\n",
    "\n",
    "# Decode the generated output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf04d40-587f-47a1-9a40-28401c8305f2",
   "metadata": {},
   "source": [
    "In this code snippet:\n",
    "\n",
    "- `output = model.generate(input_ids=tokens_pt['input_ids'], max_length=50)`: Utilizes the `generate` method of the Llama-2-Ko model to predict the next tokens in the sequence. The `input_ids` parameter is set to the PyTorch Tensor containing the input IDs obtained from tokenization (`tokens_pt['input_ids']`). The `max_length` parameter controls the maximum length of the generated sequence.\n",
    "\n",
    "- `generated_text = tokenizer.decode(output[0], skip_special_tokens=True)`: After generating the output IDs, this line uses the `decode` method of the tokenizer to convert the numerical output into human-readable text. The `output[0]` corresponds to the first sequence in the generated output, and `skip_special_tokens=True` removes any special tokens from the final decoded text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c15d5a-906d-4569-be89-f00292e918cd",
   "metadata": {},
   "source": [
    "# Closing Note\n",
    "\n",
    "In this tutorial, we've explored the fascinating world of **Large Language Models (LLMs)** with a focus on text generation using the **Llama-2-Ko base model**. It's crucial to understand that this **base model** is primarily trained to predict and complete text and is not explicitly tuned for instruction following or chat-based interactions.\n",
    "\n",
    "Let's take a quick look at an example to illustrate the behavior of this base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6893b3a2-ba6c-4e67-915f-5e8600e6e891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 인도네시아의 도시들을 인구 규모별로 정렬된 목록으로 제시해주세요.\n",
      "Generated text: 인도네시아의 도시들을 인구 규모별로 정렬된 목록으로 제시해주세요.⊙ 1960년대 1960년 4.19혁명으로 이승만 독재정권이 무너지고 장면 민주당 정권이 들어섰으나 1961년 5.16 군사정변으로 박정희 군사정권이 들어섰다. 1962년 1월 1일 시행된 '향토예비군 설치법'에 따라 19\n"
     ]
    }
   ],
   "source": [
    "# Illustrative example with the base model\n",
    "prompt = \"인도네시아의 도시들을 인구 규모별로 정렬된 목록으로 제시해주세요.\" # Give me a list of the cities in Indonesia sorted by population size.\n",
    "\n",
    "# Generate text using the base model\n",
    "output = model.generate(input_ids=tokenizer(prompt, return_tensors=\"pt\")['input_ids'], max_length=100,)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e45d7a2-53cc-42cf-a716-de3602695527",
   "metadata": {},
   "source": [
    "**Note:** The above example demonstrates that the base model generates text based on the prompt but may not strictly follow the provided instructions.\n",
    "\n",
    "In our upcoming tutorial, we will delve into the specialized capabilities of instruction-tuned and chat-tuned LLMs, exploring their ability to follow instructions and engage in chat-like interactions. Stay tuned for an in-depth journey into the tailored functionalities of these advanced language models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b7d01f-ed81-4e78-b409-5dd0c7481873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
